<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Topic: Rails | Hungry and Foolish]]></title>
  <link href="http://vladigleba.github.io/blog/topics/rails/atom.xml" rel="self"/>
  <link href="http://vladigleba.github.io/"/>
  <updated>2014-03-21T13:50:01-07:00</updated>
  <id>http://vladigleba.github.io/</id>
  <author>
    <name><![CDATA[Vladi Gleba]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Deploying Rails Apps, Part 3: Configuring Nginx and Unicorn]]></title>
    <link href="http://vladigleba.github.io/blog/2014/03/21/deploying-rails-apps-part-3-configuring-nginx-and-unicorn/"/>
    <updated>2014-03-21T10:08:00-07:00</updated>
    <id>http://vladigleba.github.io/blog/2014/03/21/deploying-rails-apps-part-3-configuring-nginx-and-unicorn</id>
    <content type="html"><![CDATA[<p>Having covered how to install the technology stack powering Phindee in my <a href="/blog/2014/03/14/deploying-rails-apps-part-2-setting-up-the-server/">part 2</a>, I will now shift gears and talk about how I configured Nginx and Unicorn. I already explained why I chose to install Nginx, but I haven’t yet explained why I chose Unicorn, so here we go.</p>

<!-- more -->


<h1>Unicorn and Passenger</h1>

<p>When I deployed Phindee for the first time, I actually used the open source version of <a href="https://www.phusionpassenger.com/">Phusion Passenger</a> due to the fact that it was (and is) easier to setup than Unicorn. My main concern, at the time, was to have a functioning app deployed as soon as possible, with as little effort as possible, and Passenger helped me do just that.</p>

<p>Eventually, I reached a point where I was ready for something that I could configure myself, and Unicorn seemed like a good next step. But if you’re a beginner, Passenger will be the easiest to start with since it’s designed to integrate into Nginx directly and, therefore, requires less work to setup and maintain. You will have to pay for the Enterprise version, however, if you want advanced features like error-resistant, zero-downtime deploys, which come for free with Unicorn.</p>

<h2>Do One Thing, Do It Well</h2>

<p>The reason why I like Unicorn is due to its philosophy of doing a few things well. An example of this is load balancing, which Unicorn hands off to the operating system entirely. When Unicorn starts, its master process spawns (forks) a configured number of processes called workers. These workers then handle the incoming requests to your app and only accept a request when they’re ready.</p>

<p>But it’s the operating system that handles the forking, as well as the distribution of requests between processes that are ready to accept, not Unicorn. What Unicorn does is the actual monitoring of workers themselves through the master process. If a worker, for example, takes too much time to complete a task, the master process will kill it and spawn a new one.</p>

<h2>Deploys Done Right</h2>

<p>What this design can achieve is error-resistant, zero-downtime deploys. Error-resistant deploys ensure that if something goes wrong during a deploy, your app will remain online and serve incoming requests using the old code. This is possible because Unicorn doesn’t kill off old workers until new workers have successfully forked, which means your old workers will stay alive if something goes wrong with the new ones.</p>

<p>Zero-downtime deploys work in a similar manner. We can send a signal to the master process telling it to start a new master, and this new master will then begin reloading our new code. Once it’s fully loaded, the new master will fork its workers. The first worker forked will notice there is still an old master running, and it’ll send a signal telling it to start gracefully shutting down its workers. When all workers finish serving their current requests, the old master then dies, and our app is fully reloaded with new code.</p>

<p>Passenger supports rolling restarts like this as well, but they only come with the paid Passenger Enterprise version. One advantage the Enterprise version provides, however, is it restarts the processes one-by-one, which requires less memory. Rolling restarts with Unicorn, on the other hand, are done all at once and temporarily require twice the memory usage. It is possible, of course, to script one-by-one rolling restarts in Unicorn, but Passenger does this automatically for you.</p>

<h1>Puma</h1>

<p>Another alternative to Unicorn and Passenger is Puma. Whereas Unicorn and Passenger achieve concurrency through the use of forks, Puma achieves it by running multiple threads in a single process. Of course, this means that your code must be thread-safe, but since Rails 4 is thread-safe by default, this shouldn’t be an issue.</p>

<p>Because threading requires less memory than forking, Puma will be more memory efficient than a similar Unicorn setup. Puma, however, does not do rolling restarts, nor does watch for and restart failed processes, like Unicorn, which means you’ll need a service like <a href="http://mmonit.com/monit/">Monit</a> that monitors and restarts them for you. As with any technology, pick whatever best meets your needs.</p>

<h1>Configuring Unicorn</h1>

<p>With that out of the way, we’re now ready to start configuring Unicorn. We’ll begin by adding the following line to our app’s <code>Gemfile</code>:</p>

<p><code>ruby Gemfile
gem 'unicorn', '~&gt; 4.8.0’
</code></p>

<p>Make sure you change the version number to whatever’s the most recent one at the time of your install. The <code>~&gt;</code> notation means that any future minor updates (e.g., from 4.0.0 to 4.0.1) will be installed, but major ones (e.g., from 4.0 to 4.1) won’t be. Major updates can sometimes introduce unexpected behavior in your app, so it’s best to limit the updates to minor releases only.</p>

<p>We can install Unicorn by running <code>bundle</code> in the root path of our app, and Bundler, which we installed in <a href="/blog/2014/03/14/deploying-rails-apps-part-2-setting-up-the-server/">part 2</a>, will take care of the install for us.</p>

<p>Having Unicorn installed, we can begin configuring it. We’ll start by creating a file called <code>unicorn.rb</code> on our local computer inside the <code>/config</code> directory of our Rails application. This is how my file for Phindee looks:</p>

<p>``` ruby unicorn.rb
root = “/var/www/phindee/current"
working_directory root
pid &ldquo;#{root}/tmp/pids/unicorn.pid&rdquo;
stderr_path &ldquo;#{root}/log/unicorn.log&rdquo;
stdout_path &ldquo;#{root}/log/unicorn.log&rdquo;</p>

<p>listen &ldquo;/tmp/unicorn.phindee.sock&rdquo;
worker_processes 2
timeout 30
```</p>

<p>The first variable <code>root</code> represents the path to the root directory, which is <code>/var/www/phindee/current</code> in my case. You can set this to whatever you like, but generally, web apps are often stored inside <code>/var/www</code> on Unix since the <code>/var</code> directory is designated for files that increase in size over time, and that’s the case with most web apps.</p>

<p>Below is what the rest of the configurations mean:</p>

<ul>
<li><p><code>working_directory</code> specifies exactly what is says&mdash;the app’s working directory&mdash; and it’s set to the variable <code>root</code>, which we just defined.</p></li>
<li><p><code>pid</code> specifies the path to a <code>.pid</code> file that will store the process ID of Unicorn’s master process, which can be later used to stop the process itself. These files are typically stored inside the <code>/tmp</code> directory since they exist only while Unicorn is running, so you can leave this line the way it is.</p></li>
<li><p><code>stderr_path</code> and <code>stdout_path</code> specify the path to <code>stderr</code> and <code>stdout</code>. If you’re not familiar with what they mean, when a Unix program starts up, it has three streams opened for it: one for input called “standard input” (abbreviated <code>stdin</code>), one for output called “standard output” (abbreviated <code>stdout</code>), and one for printing error messages called “standard error” (abbreviated <code>stderr</code>). Given our configuration, this means that any error messages written by our Rails app to the <code>stderr</code> stream will get written to the <code>.log</code> file specified in the <code>stderr_path</code>. It’s common to point <code>stdout_path</code> to the same location as <code>stderr_path</code> and store them both inside the <code>/log</code> directory.</p></li>
<li><p><code>listen</code> specifies the path to a socket that will listen for a client wanting to make a connection request. If you’re unfamiliar with this, a socket is basically a software object consisting of a port number that’s attached to an IP address. It allows clients and servers to communicate with one another by writing to and reading from their sockets. Since they’re running only when Unicorn is running, they’re usually stored inside the <code>/tmp</code> directory as well.</p></li>
<li><p><code>worker_processes</code> specifies the number of workers that the master process will fork for client request handling. The more workers you set, the more memory you’ll need, and since I don’t have a large amount of memory on my VPS, I decided to set mine to two. This should be enough for a low-traffic app, but once your traffic rises, the number of workers, as well as the amount of memory available to your server, will need to rise with it.</p></li>
<li><p><code>timeout</code> specifies the maximum number of seconds a worker can take to respond to a request before the master kills it and forks a new one. 30 seconds is a good value to put here since whenever a worker takes longer than this to respond, it’s usually safe to assume there is something wrong with the worker itself.</p></li>
</ul>


<p>You can get a complete list of all the other possible configuration options by taking a look Unicorn’s <a href="http://unicorn.bogomips.org/Unicorn/Configurator.html">Configurator Module</a>.</p>

<h1>Managing Unicorn Processes</h1>

<p>Having Unicorn configured, we’ll now need to setup a way for us to manage the Unicorn processes themselves.</p>

<p>Unicorn uses signals to communicate with its processes, and you can find a full explanation of all the available signals <a href="http://unicorn.bogomips.org/SIGNALS.html">here</a>. But sending these signals manually would be a pain. I recommend using a <a href="https://github.com/railscasts/335-deploying-to-a-vps/blob/master/blog-nginx/config/unicorn_init.sh">script on GitHub</a> to automate this process for you. Go ahead and create your own <code>unicorn_init.sh</code> file inside your app’s <code>/config</code> directory and copy/paste the script’s code into it. All the variables you can change are defined at the of the script, but for now, the only thing you’ll need to change is the <code>APP_ROOT</code> variable, which you’ll set to the same path that the <code>root</code> variable in <code>unicorn.rb</code> is set to.</p>

<p>If you’re inside the root directory of your Rails app, you can then make the script executable with the following command :</p>

<p><code>bash
chmod +x config/unicorn_init.sh
</code></p>

<p>I’d like to point out that the way <code>unicorn.rb</code> and <code>unicorn_init.sh</code> is currently setup, Unicorn won’t be doing rolling restarts. If you look at <code>unicorn_init.sh</code>, for example, you’ll notice that it sends a <code>HUP</code> signal when you run the script’s <code>restart</code> command. This signal doesn’t spawn a new master process, the way a rolling restart would do; it simply reloads the <code>unicorn.rb</code> file and gracefully restarts all the workers using the same master process.</p>

<p>You’d need to use the <code>USR2</code> signal for a rolling restart (which is actually what happens when you run the script’s <code>upgrade</code> command). But even then, there are still additional steps you’ll need to take to make everything runs smoothly, like making sure your database connections carry over, as well as ensuring any changes to the database are compatible with the older code.</p>

<p>I won’t be explaining how to do this here because I haven’t yet set it up myself, but if you’re curious, there is a good <a href="http://www.justinappears.com/blog/2-no-downtime-deploys-with-unicorn/">blog post</a> explaining all the nuances you need to be aware of. Phindee is currently a small, low-traffic app and its code is reloaded within seconds, so I’m not worried about users waiting for their requests and don’t see a need for rolling restarts at the moment, but I’m hoping the need presents itself soon.</p>

<h1>Configuring Nginx</h1>

<p>Having configured Unicorn, we can move on to configuring Nginx. We’ll start by creating a file called <code>nginx.conf</code> inside, as you might probably guess, the <code>/config</code> directory. Here’s how mine looks like:</p>

<p>``` nginx nginx.conf
worker_processes 1;</p>

<p>events {
  worker_connections 1024;
}</p>

<p>upstream unicorn {
  server unix:/tmp/unicorn.phindee.sock fail_timeout=0;
}</p>

<p>server {
  server_name www.phindee.com;
  return 301 $scheme://phindee.com$request_uri;
}</p>

<p>server {
  listen 80 default deferred;
  server_name phindee.com;
  root /var/www/phindee/current/public;</p>

<p> location ^~ /assets/ {</p>

<pre><code>gzip_static on;
expires max;
add_header Cache-Control public;
</code></pre>

<p>  }</p>

<p>  try_files $uri/index.html $uri @unicorn;
  location @unicorn {</p>

<pre><code>proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
proxy_set_header Host $http_host;
proxy_redirect off;
proxy_pass http://unicorn;
</code></pre>

<p>  }</p>

<p>  error_page 500 502 503 504 /500.html;
  keepalive_timeout 5;
}
```</p>

<p>You might have noticed that the first thing we do is specify the number of workers to run. Sounds familiar, doesn’t it? Well, that’s because Nginx, like Unicorn, also has a master process managing all the workers, and the workers are the ones responsible for processing requests from clients. Unlike Unicorn, Nginx also has a cache loader process that checks and/or populates the cache with metadata, as well a cache manager process that’s responsible for cache expiration. Together, they keep Nginx internals running quickly and efficiently.</p>

<h2>A Bit on Workers</h2>

<p>As you can see, we used the <code>worker_processes</code> directive to tell Nginx to run one worker, which is the default. We then used the <code>worker_connections</code> directive inside the <code>events</code> block to specify that the maximum number of connections a worker can accept is 1024, which is also the default. Since we’re using defaults for both directives, it wasn’t necessary to add them to our file at all, but I chose to include them for completeness.</p>

<p>Given our current configuration, our server will be able to accept a total of 1024 simultaneous connections. If you want to increase this, it’s generally best to increase the <code>worker_connections</code> value before increasing the number of workers. Remember, each worker is a single-threaded process, so whenever you increase the number of workers, you’re also increasing the total amount of memory that will be used up. Having one worker process that’s capable of handling 1024 connections is more than enough for a low-traffic app, however.</p>

<h2>Hooking up with Unicorn and Handling Redirects</h2>

<p>Since Nginx is not capable of handling requests for pages that are dynamically generated by Rails, we need to tell it to somehow pass such requests off to Unicorn. We’ll take the first step to accomplishing this by defining an <code>upstream</code> block called <code>unicorn</code>, inside which we point the server to the same Unix socket that we used in our <code>unicorn.rb</code> file. This is just the first step, however, and more work needs to be done to get this working, as you’ll see later. By the way, in case you’re wondering, setting the <code>fail_timeout</code> to 0 is necessary for Nginx to correctly handle Unicorn timing out due to its worker being killed when it takes longer than 30 seconds to respond, as specified in <code>unicorn.rb</code>.</p>

<p>The <code>server</code> block right below the <code>upstream</code> block is there to redirect a request for &ldquo;www.phindee.com&rdquo; to &ldquo;phindee.com&rdquo;. The <code>server_name</code> directive specifies the URL we’re redirecting from, while the <code>return</code> directive specifies where to redirect to. (Notice we’re returning a 301 status code to specify a permanent redirection.) The <code>$scheme</code> variable stores the HTTP scheme (i.e. http, https), while <code>$request_uri</code> stores the unmodified URI of a client request, which includes the arguments, but not the host name (e.g. &ldquo;/foo/bar.php?arg=baz&rdquo;).</p>

<h2>Where the Meat Is</h2>

<p>The next <code>server</code> block contains the main configuration. The <code>listen</code> directive inside it tells Nginx to listen on port 80, and the <code>server_name</code> directive right below specifies the domain name that Nginx will try to match, which is &ldquo;phindee.com&rdquo; in my case.</p>

<p>Specifying <code>default</code> in the <code>listen</code> directive, by the way, tells Nginx to use this server block by default if it can’t find a matching domain name, which means I could technically leave out the <code>server_name</code> directive, and everything would still work because of <code>default</code>, but I like to leave it in for readability. And finally, I added the <code>deferred</code> option since I’m running this on Linux, which tells Nginx to use the <code>TCP_DEFER_ACCEPT</code> option to <a href="http://www.techrepublic.com/article/take-advantage-of-tcp-ip-options-to-optimize-data-transmission/">speed up performance</a> by reducing the amount of preliminary work that happens between a client and the server.</p>

<p>Moving along, the <code>root</code> directive specifies the directory in which Nginx will look to handle requests for static files. Note that this is <em>not</em> our app’s root directory, but our app’s <code>/public</code> directory, and this is where our static files are/will reside. Currently, it only contains various error pages, a favicon, and a <code>robots.txt</code> file for search engines. When we later deploy with Capistrano, it’ll contain all our assets as well, including stylesheets, scripts, images, and fonts.</p>

<h2>Handling Asset Requests</h2>

<p>Just like the <code>server_name</code> directive handles requests for domain names, the <code>location</code> directive handles requests for specific files and folders. The caret and tilde (<code>^~</code>) tells Nginx to do a regular expression match on <code>/assets/</code> and to stop searching as soon as it finds a match (see the <a href="https://library.linode.com/web-servers/nginx/configuration/basic#sph_location-file-and-folder-configuration">Linode Guide</a> to learn more).</p>

<p>By setting the <code>gzip_static</code> directive to <code>on</code>, we’re then telling Nginx to look for an already pre-compressed <code>.gz</code> file <em>before</em> proceeding to compress it. This prevents Nginx from compressing the same file each time it is requested.</p>

<p>The <code>expires</code> directive then makes the response cacheable and marks it with an expiry date set to <code>max</code>, which is equivalent to December 31st, 2037. This tells browsers and any caching servers to not request these assets again until the specified date. Of course, if we make changes to our stylesheets, for example, Rails will change the filename and browsers will still receive the latest version, which will then also be cached.</p>

<p>Using the <code>expires</code> directive, however, is an outdated method of specifying caching, and it’s recommended to use <code>Cache-Control</code> header instead. The next line in the code does just that through the <code>add_header</code> directive. (The reason we include  <code>expires</code> is to make things backward-compatible.) It’s possible, by the way, to set <code>Cache-Control</code> to either <code>public</code> or <code>private</code>, and I’m setting it to <code>public</code> because we’re caching assets that are meant to be used by everybody, whereas <code>private</code> would mean they’re unique to individual users (see <a href="http://stackoverflow.com/questions/3492319/private-vs-public-in-cache-control">Stack Overflow</a> to learn more).</p>

<h2>Trying to Find a Match</h2>

<p>The next line is the <code>try_files</code> directive, which is there for requests that didn’t match with any <code>location</code> blocks. In our case, it tries to match non-asset requests. The <code>$uri</code> variable contains the current request URI, minus the arguments, protocol, and host name, so if we typed in &ldquo;phindee.com/assets&rdquo; into the address bar, the <code>$uri</code> would be &ldquo;/assets&rdquo;, and given our <code>try_files</code> directive, Nginx would try to first find an <code>/assets/index.html</code> file. If it found no such file, it would then try to find an <code>/assets</code> directory, and if that wouldn’t exist, it would then pass the request off to Unicorn through a named location, which is defined next through the <code>location</code> directive and called <code>@unicorn</code>.</p>

<p>Inside the named location, the <code>proxy_pass</code> directive does all the heavy lifting. We set it to <code>http://unicorn</code> so that it points to the <code>upstream</code> block called <code>unicorn</code>, which we already defined, and the request is then handled by the Unicorn socket defined there. The two <code>proxy_set_header</code> directives then append additional headers needed for Unicorn, while <code>proxy_redirect</code> set to <code>off</code> prevents Nginx from doing any redirect handling. (There is a sample <code>nginx.conf</code> file <a href="https://github.com/defunkt/unicorn/blob/master/examples/nginx.conf">on GitHub</a> with comments explaining why this is necessary.)</p>

<h2>Last Few Lines</h2>

<p>Alright, we’re down to the last two lines. <code>error_page</code> makes sure that our app’s <code>500.html</code> page is show for any 500-related errors, while <code>keepalive_timeout</code> tells Nginx to retain keep-alive connections (also known as persistent connections) for up to 10 seconds and close them if they exceed that time.</p>

<p>Persistent connections, by the way, are used to send multiple HTTP requests in a single connection, as opposed to opening a new connection for each request; in HTTP 1.1, all connections are persistent by default, which means stylesheets, scripts, images, and fonts, for example, would all be downloaded using a single connection.</p>

<p>These are, of course, not all the options you can specify. If you’d like to learn about the additional ones, feel free to read through the comments in the sample <code>nginx.conf</code> <a href="https://github.com/defunkt/unicorn/blob/master/examples/nginx.conf">file</a> I mentioned earlier.</p>

<h1>That Was Long</h1>

<p>I didn’t expect the post to be quite this long, and if you made all the way to the end, pat yourself on the back. But I personally love posts that explain the reasoning behind their decisions, and I tried to do the same here. I think it prepares readers to be able to make their own informed decisions in their own projects.</p>

<p>In the next and final post of this series, I will introduce Capistrano and show you how I use it to deploy Phindee to my VPS. If you want to be notified when it’s out, feel free to <a href="http://www.feedblitz.com/f/?Sub=927939&amp;cids=1">subscribe</a>, and the post will be delivered to your inbox as soon as it’s released!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploying Rails Apps, Part 2: Setting up the Server]]></title>
    <link href="http://vladigleba.github.io/blog/2014/03/14/deploying-rails-apps-part-2-setting-up-the-server/"/>
    <updated>2014-03-14T09:45:00-07:00</updated>
    <id>http://vladigleba.github.io/blog/2014/03/14/deploying-rails-apps-part-2-setting-up-the-server</id>
    <content type="html"><![CDATA[<p>In <a href="/blog/2014/03/05/deploying-rails-apps-part-1-securing-the-server/">part 1</a>, I talked about choosing a VPS provider, creating a new Ubuntu instance, and configuring it to be more secure. Now, in part 2, I&rsquo;ll talk about installing the technology stack behind <a href="http://phindee.com/">Phindee</a>: Node.js, Nginx, PostgreSQL, rbenv, Ruby, and Bundler.</p>

<!-- more -->


<h1>But First!</h1>

<p>Before we proceed any further, make sure you’re logged in as the user you created in part 1; if you’re already logged in as <code>root</code>, you can switch to the correct user with the following command:</p>

<p><code>bash
su - username
</code></p>

<p>Once logged in, we’ll run the following command to fetch the latest updates for the packages on our system:</p>

<p><code>bash
sudo apt-get update
</code></p>

<p>We’ll follow this up with the command to install the necessary package updates:</p>

<p><code>bash
sudo apt-get upgrade
</code></p>

<p>If the command found any updates to install, it will ask if you want to continue with the install; you can enter “y” to do so. Once it finishes, we’ll be ready begin.</p>

<h1>Setting Timezones and Installing Mail</h1>

<p>We’ll start by setting the correct timezone:</p>

<p><code>bash
sudo dpkg-reconfigure tzdata
</code></p>

<p>You’ll be asked to choose your country and timezone, after which your server’s local time will be displayed; if it displays the correct time, you’re good to go.</p>

<p>We’ll install <code>postfix</code> and <code>telnet</code> next to enable our Rails app to send email:</p>

<p><code>bash
sudo apt-get -y install telnet postfix
</code></p>

<p>Feel free to just press “enter” through all the prompts and keep all the defaults.</p>

<p>Next, we’ll install some useful packages we’ll later need, among them <code>python-software-properties</code>, which will allow us to easily add new repositories to the <code>apt</code> package handling system:</p>

<p><code>bash
sudo apt-get -y install curl git-core python-software-properties
</code></p>

<p>Having the ability to add new repositories this way allows us to install the most recent updates since the default <code>apt-get</code> repositories typically don’t receive the latest updates immediately.</p>

<h1>Installing Node.js</h1>

<p>We’ll actually put this ability to use right now by adding a new repository for <a href="http://nodejs.org/">Node.js</a>:</p>

<p><code>bash
sudo add-apt-repository ppa:chris-lea/node.js
</code></p>

<p>We’ll then update the created repository with the latest Node.js code available:</p>

<p><code>bash
sudo apt-get -y update
</code></p>

<p>and install it, like so:</p>

<p><code>bash
sudo apt-get -y install nodejs
</code></p>

<p>We could’ve avoided adding a new repo and just used the traditional <code>apt-get</code> method to do the install, but this would’ve installed an older version of Node.js. Because Node.js is under active development and things are added quite frequently, it’s important to run the latest possible version. This might not matter as much for software that doesn’t have an aggressive update schedule, but this is the route we’ll take for Node.js.</p>

<p>By the way, if you’re wondering why we’re installing Node.js, the reason is it provides a good way to execute JavaScript, and we’ll need this for the Rails <a href="http://guides.rubyonrails.org/asset_pipeline.html">asset pipeline</a>.</p>

<h1>Installing Nginx</h1>

<p>Next, we’ll install a web server called <a href="http://wiki.nginx.org/Main">Nginx</a>, which will handle all our static requests, such as stylesheets, scripts, images, and fonts. Its low memory usage and ability to serve static content quickly and efficiently make it a popular alternative to Apache and an excellent choice for sites running on a Virtual Private Server (VPS). What makes Nginx efficient is the fact that it’s an event-based server, while Apache, on the other hand, is process-based. An event-based server doesn&rsquo;t spawn new processes or threads for each request the way a process-based one does, and this means lower memory usage and faster responses.</p>

<p>We’ll install it by adding another repository:</p>

<p><code>bash
sudo add-apt-repository ppa:nginx/stable
sudo apt-get -y update
sudo apt-get -y install nginx
</code></p>

<p>Once it’s installed, we can start it up with:</p>

<p><code>bash
sudo service nginx start
</code></p>

<p>If you now visit your server’s IP address, you should see a simple page proclaiming “Welcome to nginx!”</p>

<h1>Installing PostgreSQL</h1>

<p>Most modern apps need to store some sort of data, and there are a plethora of open source databases available, like <a href="https://www.mysql.com/">MySQL</a>, <a href="https://sqlite.org/">SQLite</a>, and <a href="http://www.postgresql.org/">PostgreSQL</a>. I never tried MySQL, but when I first started out, I used SQLite, the default database for Rails apps, because I liked its simplicity and saw no need for something more sophisticated. As my needs have evolved, however, so has my database, and I recently decided to switch to PostgreSQL because of its support for a fast key-value store called HStore and its ability to do full-text search, both of which I&rsquo;ll need for Phindee.</p>

<p>We’ll install it with <code>apt-get</code>:</p>

<p><code>bash
sudo apt-get install postgresql postgresql-contrib
</code></p>

<p>We can then start Postgres as the default <code>postgres</code> user with the following command:</p>

<p><code>bash
sudo -u postgres psql
</code></p>

<p>Had we not specified the default user, it would’ve tried to use the user we’re logged in with on our VPS, and Postgres would’ve complained that the role doesn’t exist since there is no such user created in Postgres. This makes it necessary to login as the default <code>postgres</code> user.</p>

<p>Once logged in, we’ll setup a password for <code>postgres</code>:</p>

<p><code>sql
\password
</code></p>

<p>We’ll also create a new user called <code>admin</code>, followed by a database called <code>phindee</code>, which will be owned by <code>admin</code>:</p>

<p><code>sql
create user admin with password 'secret';
create database phindee owner admin;
</code></p>

<p>Having the basics setup, we can now quit Postgres:</p>

<p><code>sql
\quit
</code></p>

<h1>Installing rbenv</h1>

<p><a href="https://github.com/sstephenson/rbenv">rbenv</a> is a tool that helps you manage the Ruby versions installed on your system, thereby allowing you to easily switch between them. Whenever you want to play with a new version of Rails&mdash;without messing up your current setup&mdash;rbenv will come in handy.</p>

<p>You may be familiar with another Ruby version manager called <a href="https://rvm.io/">RVM</a>. I used it myself for a while, before switching over to rbenv. It’s not that one is “better” than the other; it’s about which one is better suited for <em>your</em> needs. I made the switch because rbenv is more lightweight than RVM, its design feels cleaner, and it has a cool name.</p>

<p>rbenv will suite you well if you’re starting out; otherwise, install whatever best meets your needs. By the way, it’s worth mentioning that since rbenv is incompatible with RVM, you won’t be able to run them side by side.</p>

<p>Alright, we can install rbenv like so:</p>

<p><code>bash
sudo curl -L https://raw.github.com/fesplugas/rbenv-installer/master/bin/rbenv-installer | bash
</code></p>

<p>This will run a script that will do most of the install for us. In the end, you’ll receive a message telling you to add rbenv to the load path, and you can do so by opening up <code>bash_profile</code>:</p>

<p><code>bash
sudo nano ~/.bash_profile
</code></p>

<p>and copying/pasting the code that was outputted by the message. We’ll then need to reload the file for the changes to take effect:</p>

<p><code>bash
. ~/.bash_profile
</code></p>

<p>We’re almost ready to install Ruby, but before we do, we first need to install the C compiler and the Make utility, which is needed for the Ruby install. We can do so by installing a package called <code>build-essential</code>, along with some additional packages we’ll need later on:</p>

<p><code>bash
sudo apt-get install zlib1g-dev build-essential libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libpq-dev
</code></p>

<p>With the packages installed, we’re now ready to install Ruby itself.</p>

<h1>Installing Ruby</h1>

<p>To see a list of all the Ruby versions available, we can run the following command:</p>

<p><code>bash
rbenv install --list
</code></p>

<p>I chose to install version 2.1.0, as that was the latest one at the time:</p>

<p><code>bash
rbenv install 2.1.0
</code></p>

<p>This will take a few minutes to run&mdash;and that’s probably an understatement&mdash;but once it finishes, we’ll make the version it just installed the default Ruby version on our server:</p>

<p><code>bash
rbenv global 2.1.0
</code></p>

<p>If everything finished successfully, typing <code>ruby -v</code> should output the Ruby version we now have installed.</p>

<h1>Installing Bundler</h1>

<p>If you’ve never used it before, <a href="http://bundler.io/">Bundler</a> is a tool that helps you easily manage and install gems (Ruby programs and libraries). It allows you to specify the gems your app relies on, along with their versions, and Bundler will then install them all for you, in addition to automatically installing and managing any dependencies (other gems) they rely on.</p>

<p>It’s usually a good idea to include version numbers for your gems because new versions can sometimes introduce changes that cause the old features you rely on to behave differently, which can result in errors the next time you try to run your app. By using Bundler to specify not only the gems you need, but also the versions of those gems, you can save yourself from needless headaches (and unnecessary cups of coffee).</p>

<p>We will install bundler with the following command:</p>

<p><code>bash
gem install bundler --no-ri --no-rdoc
</code></p>

<p>Every time we install a gem that provides us with commands we can execute, we’ll need to run <code>rbenv rehash</code>, which will give us access to the corresponding executable (<a href="http://stackoverflow.com/questions/9394338/how-do-rvm-and-rbenv-actually-work">see this page</a> to learn why this is so). Since Bundler is one of these gems, we’ll do the rehash next:</p>

<p><code>bash
rbenv rehash
</code></p>

<p>If things installed successfully, <code>bundle -v</code> should return the Bundler version that was just installed.</p>

<p>As an aside, notice that we’re specifying the <code>—no-ri</code> and <code>—no-rdoc</code> flags to avoid installing the gem’s documentation, which often takes longer than the gem installation itself and is typically unnecessary, especially on a production server. But typing out these flags for each and every gem you install will give you <a href="http://www.webmd.com/pain-management/carpal-tunnel/carpal-tunnel-syndrome-topic-overview">carpel tunnel</a> sooner than you&rsquo;d like, so its best to create a <code>.gemrc</code> file in your home directory:</p>

<p><code>bash
nano ~/.gemrc
</code></p>

<p>and add the following line into it:</p>

<p><code>text
gem: –no-ri –no-rdoc
</code></p>

<p>The flags will then be included automatically the next time you install new gems.</p>

<p>And with that, our server setup is now complete! Having installed Node.js, Nginx, PostgreSQL, and rbenv, we’re now ready to start configuring Nginx and Unicorn, which I’ll cover in the <a href="/blog/2014/03/21/deploying-rails-apps-part-3-configuring-nginx-and-unicorn/">next post</a>. If you want to be notified when it’s out, feel free to <a href="http://www.feedblitz.com/f/?Sub=927939&amp;cids=1">subscribe</a>, and you’ll have the complete post delivered to your inbox as soon as it’s released!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploying Rails Apps, Part 1: Securing the Server]]></title>
    <link href="http://vladigleba.github.io/blog/2014/03/05/deploying-rails-apps-part-1-securing-the-server/"/>
    <updated>2014-03-05T11:18:00-08:00</updated>
    <id>http://vladigleba.github.io/blog/2014/03/05/deploying-rails-apps-part-1-securing-the-server</id>
    <content type="html"><![CDATA[<p>Setting up a Rails server from scratch can be a daunting task. I remember my first attempt; it was a multi-day process full of frustration, things not working, me not understanding why, and a whole lot of googling. In an effort to make this experience less painful for those new to Rails, I’d like to share the process I went through to deploy <a href="http://phindee.com/">Phindee</a> to a VPS (Virtual Private Server).</p>

<!-- more -->


<h1>Choosing a VPS</h1>

<p>Phindee is currently running on DigitalOcean servers, but there are other options available as well, like Linode, which was my previous VPS provider. If you’re new to deployment, I recommend <a href="http://digitalocean.com/">DigitalOcean</a> because it’ll be ideally suited to your needs, due their more flexible billing policy and cheaper plans, but any VPS provider will do.</p>

<p>Once you decide on a VPS, you’ll then signup for a plan. If you’re just starting out, the cheapest plan available will be enough; otherwise, choose a plan that fits your needs. Once you have an account, you’ll be able to create your server, and typically, you’ll have a list of operating systems to choose from. DigitalOcean offers a wide variety of Linux distributions; I chose the latest 32-bit version of Ubuntu for Phindee, and I recommend you do the same if you&rsquo;re new to deployment.</p>

<p>The reason why I chose the 32-bit version was because it uses less memory than the 64-bit one. This is something you should consider if you chose one of the cheaper plans with a lower amount of memory, but if memory is not an issue, go with the 64-bit since you’ll have better performance (<a href="howtoubuntu.org/how-to-decide-if-you-should-use-32bit-or-64bit-ubuntu">see this page</a> to learn more).</p>

<h1>Logging In</h1>

<p>Once you create your instance, you’ll be given your server’s IP address and password. If you’re on Linux or a Mac, open up Terminal and login. (If you&rsquo;re on Windows, you&rsquo;ll need to download Putty.) To login using Terminal, use the following command, replacing the Xs with your own IP address:</p>

<p><code>bash
ssh root@xxx.xxx.xxx.xxx
</code></p>

<p>This command uses SSH to connect to your server as the user <code>root</code>. If you’re unfamiliar with SSH, it stands for Secure Shell, and it’s basically a network protocol that allows two computers to securely communicate with one another. There are many other protocols out there, such as HTTP, which allows browsers to communicate with web servers.</p>

<p>The first time you attempt to login, you’ll be asked if you’re sure you want to continue connecting; type &ldquo;yes&rdquo;. Then enter the password for the <code>root</code> user, and you’ll be logged in.</p>

<h1>Groups and Privileges</h1>

<p>Now that you’re in, the first thing we’ll do is change the password for <code>root</code> using the following command:</p>

<p><code>bash
passwd
</code></p>

<p>This will prompt you to enter a new password twice. Next, we’ll create a new group called <code>deployers</code>, which will allow us to easily manage the users with deployment privileges:</p>

<p><code>bash
groupadd deployers
</code></p>

<p>Now we’ll create a new user called <code>bob</code>, and assign him to the <code>deployers</code> group we just created above:</p>

<p><code>bash
adduser bob —ingroup deployers
</code></p>

<p>This command will prompt you to enter a password for this user, in addition to some other information afterwards, but after you enter the password twice, feel free to just press “Enter” for the other fields, as they’re not strictly necessary. By the way, don’t use the same password for both <code>root</code> and the user you just created above or <a href="http://www.cartoonstock.com/lowres/computers-computer-self_destruct-explode-username-password-ksm0529l.jpg">bad things will happen</a>.</p>

<p>Next thing we’ll do is open the <code>sudoers</code> file containing a list of users and groups who have root privileges:</p>

<p><code>bash
nano /etc/sudoers
</code></p>

<p>and we’ll add the following line into it:</p>

<p><code>text sudoers
%deployers      ALL=(ALL) ALL
</code></p>

<p>You can then exit the nano text editor by typing &ldquo;Control-X&rdquo; and typing &ldquo;Y&rdquo; when asked if you want to save. In case you’re wondering, the line we just added above will give the users in the <code>deployers</code> group the ability to run commands as <code>root</code>. If this is new to you, I can explain.</p>

<p>Running commands while logged in as <code>root</code> is considered bad practice because, as the superuser, <code>root</code> can run any and all commands, and since there is no undo functionality in Unix, one accidental bad command and your system can be seriously disrupted. That’s why we created a separate user called <code>bob</code>, which will have deployment privileges and nothing else.</p>

<p>But why did we create a <code>deployers</code> group and added <code>bob</code> into it? Well, first of all, we could’ve avoided creating a group altogether and just added <code>bob</code> to the <code>sudoers</code> file and given <em>him</em> <code>root</code> privileges instead. But let’s say I’m working on a project with a friend and she wants to be able to deploy as well. I would have to then add her to the <code>sudoers</code> file too (to give her <code>root</code> privileges), and the file would keep growing every time a new user with deployment privileges needed to be added. This would be a nightmare to maintain.</p>

<p>A better way to go about this is to create a group called <code>deployers</code>, give the group <code>root</code> privileges, and then add users to this group. This way, whenever I’d need to add new users with deployment privileges, I would just need to add them to the <code>deployers</code> group. This keeps the <code>sudoers</code> file clean and organized, while allowing me to easily manage the members of the group as well. I could, for example, easily revoke some rights for all members of the <code>deployers</code> group at the same time, instead of doing it one user at a time, or I could simply remove a user from the <code>deployers</code> group if I discover, for example, that he still creates &ldquo;1234&rdquo; passwords for his accounts.</p>

<p>Okay, but why is it necessary for users and groups to have <code>root</code> privileges? Well, these privileges allow a user, say <code>bob</code>, to run commands he otherwise would not be able to run due to not having the necessary permissions, which arises from the fact that the user is not <code>root</code> and therefore has limited privileges. But given <code>root</code> privileges, or being part of a group with <code>root</code> privileges, enables <code>bob</code> to run these commands simply by preceding the command with <code>sudo</code>. He’ll then be prompted to enter his password, and the command will run.</p>

<p>That’s the reasoning behind giving the <code>deployers</code> group <code>root</code> privileges and adding <code>bob</code> into it. Later on, <code>bob</code> will need these privileges during the deployment process.</p>

<h1>Configuring SSH Access</h1>

<p>Now we’re ready for the next step in securing our server, and we’ll start by opening the <code>ssh_config</code> file:</p>

<p><code>bash
nano /etc/ssh/sshd_config
</code></p>

<p>This file contains a number of rules that define who can login to the server and in what way. The first thing we’ll do is change the port number with which users will login; the default port that servers listen on is 22, but it’s wise to change it to another value so that any potential hackers have some extra work to do in figuring out the correct one; you can choose any port number from 1025 to 65536. Once you have your number, look for a line that looks like the following:</p>

<p><code>text sshd_config
Port 22
</code></p>

<p>and change its port number to the one you picked. Make sure you make a note of the new port number because you’ll need it for future login.</p>

<p>Next, look for another line in the file that looks like this:</p>

<p><code>text sshd_config
PermitRootLogin yes
</code></p>

<p>and change the “yes” to a “no”; this prevents <code>root</code> user login, which means that any potential hackers will need to know the name of one of the users on the server to actually login.</p>

<p>We can even go a step further and define exactly which existing users are able to login. Since I only want <code>bob</code> to have login access, I’ll add the following line to the end of the file:</p>

<p><code>text sshd_config
AllowUsers bob
</code></p>

<p>You could even specify a space-separated list of users here, if you have more than one user in need of login access.</p>

<p>Alright, there is one final line that we’ll add to the end of our file:</p>

<p><code>text sshd_config
UseDNS no
</code></p>

<p>This line disables hostname lookup, which can lead to a delay of up to 30 seconds when logging in with <code>ssh</code>. Disabling it will save you time and do no harm.</p>

<p>To put these changes into effect, we’ll reload SSH, like so:</p>

<p><code>bash
/etc/init.d/sshd reload
</code></p>

<p>Now we’re ready to test the configurations we just made to make sure they work. I’ll open a new shell in Terminal, without closing my current one, and try to login as the user <code>bob</code> on the port I specified in <code>sshd_config</code>:</p>

<p><code>bash
ssh -p 23523 bob@xxx.xxx.xxx.xxx
</code></p>

<p>Make sure you change the above command to match the user and port number you specified in your own <code>sshd_config</code> file, or it obviously won’t work. The above command will then prompt you to enter that user’s password. If you login successfully, congratulations! Your configuration is correct! You can close your previous shell and just continue using the current one; otherwise, you’ll need to go back and double check your <code>sshd_config</code> file configurations.</p>

<h1>Enabling SSH Authentication</h1>

<p>The final thing we’ll do to secure our server is enable SSH authentication, which will allow us to use SSH keys to authenticate with the server, instead of the traditional password authentication. This is a more secure approach because password authentication involves sending your password over the network, and this makes it vulnerable to being intercepted and cracked. It’s also more convenient since you won’t need to enter it every time you want to login. But before we move on, I’d like to briefly explain how SSH keys work and what makes them more secure.</p>

<p>All SSH keys come in pairs: one private and the other public. The private key is stored locally and needs to be carefully guarded, while the public key is stored on the remote server to which you will be logging in. Anytime you want to connect to the server, it will use the public key to create a challenge, which it will then send over to you, and only you, the holder of the private key, will be able to correctly understand and solve the challenge. Your response is then sent back to the server, and if it’s correct, it’ll grant you access.</p>

<p>If you don’t already have an SSH key, you can generate it with the following command:</p>

<p><code>bash
ssh-keygen
</code></p>

<p>It’ll prompt you to enter a path and passphrase, but the default path is fine, and since we won’t be setting up a passphrase, you can just press “enter” for both. This will store both the private and public keys in the <code>~/.ssh/</code> directory, and they will be named according to the type of encryption used, the default being RSA authentication. Your private key will be stored in a file called <code>id_rsa</code>, while <code>id_rsa.pub</code> will hold your public key.</p>

<p>Having our keys generated, we’re now ready to copy our public key over to the remote server using the <code>ssh-copy-id</code> command. (If you’re on a Mac, and you don’t have <code>ssh-copy-id</code> installed, you can install it using Homebrew with <code>brew install ssh-copy-id</code>.) Below is the full <code>ssh-copy-id</code> command that will copy our key over to the server:</p>

<p><code>bash
ssh-copy-id -i ~/.ssh/id_rsa.pub -p 23523 bob@xxx.xxx.xxx.xxx
</code></p>

<p>This will create a new file called <code>authorized_keys</code> on your remote server inside the <code>~/.ssh</code> directory and store your public key in it. If you now try to <code>ssh</code> into your server, you should be authenticated and logged in without entering your password.</p>

<p>Going through this process might seem a bit tedious and time consuming at first, but after you’ve done it a couple times, it will get easier and hopefully become second nature. Security is important, and the time you spend learning and implementing it is time well spent.</p>

<h1>Summary</h1>

<p>To summarize, we made our server more secure by:</p>

<ol>
<li>limiting <code>root</code> privileges to just members of the <code>deployers</code> group</li>
<li>setting a custom port with which to connect</li>
<li>disabling <code>root</code> login</li>
<li>specifying exactly which user is able to login</li>
<li>enabling SSH authentication</li>
</ol>


<p>Of course, this doesn’t mean our server is “unhackable” by any means, but it is significantly more secure than it was before. You can now sleep more peacefully knowing that any future hackers have at least some of their work cut out for them.</p>

<p>In <a href="/blog/2014/03/14/deploying-rails-apps-part-2-setting-up-the-server/">part 2</a>, we’ll start setting up the server by installing the technology stack behind Phindee. If you’d like to be notified when its out, feel free to <a href="http://www.feedblitz.com/f/?sub=927939">subscribe</a>, and you&rsquo;ll get the complete post delivered right to your inbox as soon as it&rsquo;s released.</p>

<p>Stay hungry. Stay foolish.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing Ruby on Rails Apps]]></title>
    <link href="http://vladigleba.github.io/blog/2014/01/17/testing-ruby-on-rails-apps/"/>
    <updated>2014-01-17T12:35:00-08:00</updated>
    <id>http://vladigleba.github.io/blog/2014/01/17/testing-ruby-on-rails-apps</id>
    <content type="html"><![CDATA[<p>I started learning Ruby on Rails over a year ago, and as most beginners, I chose the popular <cite><a href="http://ruby.railstutorial.org/ruby-on-rails-tutorial-book">Ruby on Rails Tutorial</a></cite> as my initial guide. Because there was so much new material to absorb, I decided to skip the sections about testing (and I’m glad I did or my head would’ve exploded). When I finished the book, I decided to build a Rails app called <a href="http://phindee.com/">Phindee</a> in order to solidify what I had just learned. I never went back to learn about testing, however. Now over a year later, I did just that and was finally able to write a solid test suite for the app.</p>

<!-- more -->


<p>To be honest, I was a bit reluctant to pickup testing at first. I knew it was important to test code (and I did that by sprinkling <code>print</code> statements all over my code), but it was never a part of my workflow. When I got into the zone, the last thing I wanted to do is slow down and write tests. Over the past couple of days, however, I finally saw the light. And it was glorious.</p>

<p>Let me share it with you.</p>

<h1>Test-Driven Development</h1>

<p>Test-driven development (TDD) is an approach to software development in which we first write a test for a desired functionality, then run the test to make sure it fails, and only then do we implement the said functionality. Once implemented, we run the test once more to make sure our implementation behaves the way our test says it should.</p>

<p>We write and run a failing test first for two reasons:</p>

<ol>
<li>it helps guide our implementation due to the fact that we’ve already identified what the result should look like, and</li>
<li>it makes sure that the test is actually covering the functionality we think it is, because it’s easy to write a test that doesn’t really check what we think its checking</li>
</ol>


<p>The benefits of TDD are many, but the way I see it, it boils down to three main ones: peace of mind, saved time, and better code.</p>

<h2>Peace of Mind</h2>

<p>How many times have you found yourself wanting to refactor an ugly mess of code, but due to the fear of breaking things, you ended up ditching the effort all together? This happens to me all the time, and I hate it. It doesn’t need be this way though. Since adding test cases to Phindee, I’ve refactored more than half of my helper functions without any worry of breaking things.</p>

<p>But it gets better. Testing not only allows you to refactor with confidence, you also get to deploy with confidence, and this comes as part of the package, without any additional effort.</p>

<p>This kind of peace of mind is possible because a test suite catches bugs in your code like no other. You don’t even need to write a large number of tests to reap the rewards; a few well-written test cases can go a long way.</p>

<h2>Saved Time</h2>

<p>Let me ask you this: Would you rather run a command that looks for bugs in your code on demand, and tells you exactly where to look if it finds them, or would you rather have your users discover the bugs in production, thereby sending you on a frantic bug-hunting spree? It’s a no brainer, yet all too often we find ourselves discovering bugs in production when they could’ve easily been discovered in development.</p>

<p>The beauty with having a test suite is you write your tests once, and running them on demand is as simple as typing a short command. The amount of time this saves is enormous. Of course, I’m not saying that writing test cases means you’re production environment will be bug free because software is never bug free; but it <em>will</em> help you track down <em>most</em> bugs <em>before</em> they reach production and do so in a fraction of the time it would’ve taken otherwise.</p>

<h2>Better Code</h2>

<p>Now that Phindee is backed up by a solid test suite, my code has drastically improved in quality because I was finally able to refactor it. It’s simpler, and there is now less of it.</p>

<p>Furthermore, having to write test cases for individual methods has also forced me to write simpler, decoupled methods. You see, it’s hard to write test cases for methods that do more than one thing and happen to be entangled with one another. And this is the reason why test cases lead to cleaner, simpler code. As a result, tracking down bugs is even easier, which means more saved time.</p>

<h1>How It’s Done</h1>

<p>Now that we’ve covered the benefits, I’d like to show you how easy it is to do the testing. Note that I will be using the testing library called Test Unit that ships by default with Rails, instead of the RSpec framework used by the <cite>Ruby on Rails Tutorial</cite>. (I’ll discuss why a bit later.)</p>

<p>Rails provides directories for five different categories of tests by default: helper tests, unit tests (directory is called <code>models</code>), functional tests (directory is called <code>controllers</code>), mailer tests, and integration tests. But before I go into them, I first need to introduce fixtures.</p>

<h2>Fixtures</h2>

<p>Fixtures are defined in YAML files, and their role is to initialize our models with sample data for the sole purpose of testing. They allow us to then easily use this data in our test cases without corrupting our development database. As an example, below is a fixture file for a model in Phindee called <code>Place</code>:</p>

<p>``` yaml places.yml
thai:
  name: Thai Chili Jam
  website: thaichilijam.com</p>

<p>grill:
  name: Portland Sports Bar and Grill
  website: portlandsportsbarandgrill.com
```</p>

<p>Here I created two instances of the <code>Place</code> model (<code>thai</code> and <code>grill</code>) and initialized their <code>name</code> and <code>website</code> attributes. The data is now ready to be used in our test cases. Because YAML is beyond the scope of this post, I won’t go into any more detail, but I encourage you to <a href="http://yaml.org/YAML_for_ruby.html">learn more</a>.</p>

<p>Now that we know about fixtures, we’re ready to learn about the different types of tests we can write for a Rails app. To better explain each type, I will show examples from Phindee.</p>

<h2>Helper Tests</h2>

<p>Helper tests are just what they sound like&mdash;they’re tests for your helper methods. When you create a controller using the <code>rails generate controller NAME</code> command, Rails automatically creates a <code>NAME_helper_test.rb</code> file inside <code>test/helpers</code> to write the tests in. Below is what one of my helper tests for Phindee looks like:</p>

<p>``` ruby happy_hours_helper_test.rb
  &hellip;</p>

<p>  test &lsquo;should return days given integers&rsquo; do</p>

<pre><code>assert_equal humanize_days('2'), 'monday'
assert_equal humanize_days('1-5'), 'sunday-thursday'
assert_equal humanize_days('3,4,7'), 'monday, wednesday, saturday'
</code></pre>

<p>  end</p>

<p>  &hellip;
```</p>

<p>The <code>assert_equal</code> method makes sure that <code>humanize_days(‘2’)</code> returns a string with a value of <code>’monday’</code>; if it doesn’t, it will raise an error. Because the <code>humanize_days</code> method understands three different string formats, I test each one once. If one of the three calls fails, it will tell me exactly which one failed, thereby making debugging easier. All it takes is three lines of code, and my method is fully tested.</p>

<p>In practice, we would typically first write these tests, run them to make sure they’re failing, and only then would we start their implementation.</p>

<h2>Unit Tests</h2>

<p>Unit tests are there to test your models. The <code>rails generate model NAME</code> command creates a file for these tests called <code>NAME_test.rb</code> inside the <code>test/models</code> directory. Below are two tests from Phindee for an attribute called <code>location_id</code>:</p>

<p>``` ruby happy_hour_test.rb
  &hellip;</p>

<p>  def setup</p>

<pre><code>@place = places(:thai) 
</code></pre>

<p>  end</p>

<p>  test &lsquo;should be invalid if name is missing&rsquo; do</p>

<pre><code>@place.name = nil
assert !@place.valid?
</code></pre>

<p>  end</p>

<p>  test &lsquo;should be invalid if name exceeds max length&rsquo; do</p>

<pre><code>@place.name = 'a' * 51
assert !@place.valid?
</code></pre>

<p>  end</p>

<p>  test &lsquo;should be invalid if name is not unique&rsquo; do</p>

<pre><code>identical = @place.dup
assert !identical.valid?
</code></pre>

<p>  end</p>

<p>  &hellip;
```</p>

<p>The <code>setup</code> method is not an actual test case; it’s just a method that gets called before each test case is executed. It simply initializes an instance variable called <code>@place</code> with the fixture we defined earlier called <code>thai</code>. This makes the <code>@place</code> instance variable available inside each subsequent test case.</p>

<p>The first test case sets the <code>name</code> attribute to <code>nil</code> and calls the <code>assert</code> method to check that the <code>valid?</code> method returned <code>false</code>. In other words, it&rsquo;s checking for the line below:</p>

<p><code>ruby place.rb
validates :name, presence: true
</code></p>

<p>The second test makes sure that a <code>name</code> attribute that exceeds the maximum length of 50 characters is not valid. This means it will look for a <code>length</code> helper with a <code>maximum</code> value set to 50, like so:</p>

<p><code>ruby place.rb
validates :name, presence: true, length: { maximum: 50 }
</code></p>

<p>And finally, the third test makes sure that duplicates are not valid, which means it&rsquo;ll look for a <code>uniqueness</code> helper set to <code>true</code>:</p>

<p><code>ruby place.rb
validates :name, presence: true, length: { maximum: 50 }, uniqueness: true
</code></p>

<p>You may be wondering what’s the point of all this? Well, if you ever accidentally delete a uniqueness declaration, for example, the test suite will let you know, and you will be able to fix it before you push your code to production and wreak havoc in your database.</p>

<h2>Functional Tests</h2>

<p>Functional tests are there to test your controllers, although you can also use them to test your views and verify that important HTML elements are present. Running <code>rails generate controller NAME</code> creates a file for these tests called <code>NAME_controller_test.rb</code> inside <code>test/controllers</code>. Let’s look at an example:</p>

<p>``` ruby happy_hour_controller_test.rb
  &hellip;</p>

<p>  test &ldquo;should get happening_now&rdquo; do</p>

<pre><code>get :happening_now          # simulates a get request on happening_now action
assert_response :success    # makes sure response returns with status code 200

# variables
assert_not_nil assigns(:happening_now)
assert_not_nil assigns(:geojson)

# header
assert_select '.intro h1', 'phindee' 
assert_select '.intro p', /.+/  # regex makes sure element is not empty

# definition list
assert_select 'article dl img', count: 2  # must be two img elements

# list items
assert_select 'article li p', /#{humanize_hours(assigns(:happening_now).first.start_time)}/
assert_select 'article li h2', assigns(:happening_now).first.location.place.name
</code></pre>

<p>  end</p>

<p>  &hellip;
```</p>

<p>The <code>assert_not_nil</code> method makes sure the variable that the <code>assigns</code> method retrieves is actually initialized. Note that <code>:happening_now</code> and <code>:geojson</code> are instance variables inside the controller, but here they&rsquo;re symbols.</p>

<p>All the other remaining assertions use the <code>assert_select</code> method to select an HTML element using the familiar CSS syntax and make sure it’s value is what we expect it to be. As you can see, the method is quite powerful; it can check for a specific string, evaluate a regular expression, and check for a certain number of elements using the <code>count</code> method, among <a href="http://api.rubyonrails.org/classes/ActionDispatch/Assertions/SelectorAssertions.html">other things</a>.</p>

<p>I’m only scratching the surface here of what’s possible with functional tests, and I encourage you to check out the official <a href="http://guides.rubyonrails.org/testing.html">Rails guide on testing</a> to learn more.</p>

<h2>Mailer Tests</h2>

<p>As you might guess, mailer tests are there to test mailer classes. A <code>NAME_mailer_test.rb</code> file is created inside <code>test/mailers</code> anytime you run <code>rails generate mailer NAME</code>. You can test your mailers in two different ways:</p>

<ol>
<li>test the mailer in isolation to make sure its output is what you expect (using unit tests)</li>
<li>test the controllers and models that use the mailers to make sure the right email is sent at the right time (using functional tests)</li>
</ol>


<p>When testing your mailers with unit tests, you’ll use fixtures to provide sample data demonstrating how the output should look. I don’t have any examples of mailer tests to show because I have not yet needed to implement email functionality for Phindee, but the <a href="http://guides.rubyonrails.org/testing.html">Rails guide</a> should give you a good feel for what they look like.</p>

<h2>Integration Tests</h2>

<p>Last but not least, we have integration tests, which are used to test controllers interacting with one another; they’re the “big picture” tests that make sure important workflows within your application are as bug free as possible. I haven’t written any integration tests for Phindee either because the app is simple enough that I only need one controller currently, but that will change in the near future, and I will update this section accordingly; in the meantime, feel free to see the <a href="http://guides.rubyonrails.org/testing.html">Rails guide</a> for examples.</p>

<p>One final thing I’d like to mention is the <code>test/test_helper.rb</code> file, which holds the default configuration for our tests. This file is included in all the tests, which means any methods added here are automatically available in all our tests. Pretty neat.</p>

<h1>Why Not RSpec?</h1>

<p>I chose not to use RSpec because I wanted learn about the way testing is done in Rails by default and see how it compares with RSpec. So far, it seems like both approaches are equally capable of doing everything necessary to sufficiently test your code; they just take a different approach with regards to the way you <em>write</em> the tests. RSpec&rsquo;s syntax seems more verbose and reads like English, while Test Unit’s syntax is more terse.</p>

<p>Currently, I’m leaning towards Test Unit because its terse syntax means less typing, and since it comes baked in with Rails, there is no need to inflate the code base with additional gems. (Rails 4 actually incorporated a library called MiniTest into Test Unit, which now offers support for RSpec-like syntax.)</p>

<p>But all this is irrelevant because what truly matters is that you practice test-driven development. Hopefully, I’ve shown you how easy it is to do it and convinced you that the benefits of doing so more than make up for the effort of writing them.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using @font-face with Ruby on Rails]]></title>
    <link href="http://vladigleba.github.io/blog/2013/11/29/using-at-font-face-with-ruby-on-rails/"/>
    <updated>2013-11-29T11:35:00-08:00</updated>
    <id>http://vladigleba.github.io/blog/2013/11/29/using-at-font-face-with-ruby-on-rails</id>
    <content type="html"><![CDATA[<p>When I was working on <a href="http://phindee.com/">Phindee</a>, I struggled with adding custom fonts; the asset pipeline had just been released, and it changed how one worked with assets. Perhaps there are others out there struggling with the same thing, which is why I’d like to share what I did and hopefully save some time for a few.</p>

<!-- more -->


<p>I&rsquo;ll be using <a href="http://www.fontsquirrel.com/">Font Squirrel</a> to generate the necessary font files, but there are other alternatives out there like <a href="https://www.google.com/fonts/">Google Fonts</a> and <a href="http://typekit.com/">Typekit</a>, which are easier to set up because they host the fonts for you on their own servers. The down side is if their servers go down so do your fonts. With Font Squirrel this is not the case because the fonts live directly on your own servers. This makes the setup a bit more involved, but hey, it&rsquo;s a learning opportunity, and the pay off is well worth it.</p>

<p>Alright, let&rsquo;s get to work.</p>

<h1>Setup</h1>

<p>First thing we’ll do is add a new directory called “fonts” to the <code>app/assets</code> directory; this is where we’ll place all our font files. I usually use Font Squirrel to generate these files, as they have hundreds of open source fonts to choose from; if you find a font you like, you can get access to the font files by downloading the font’s Webfont Kit, which includes all four major font formats (TTF, EOT, WOFF, and SVG). This means your fonts will be cross-browser compatible, as each major browser now supports at least one of the four formats.</p>

<p>Once we have our font files downloaded, we’ll add them to <code>app/assets/fonts</code>.</p>

<h1>Declaring Your Fonts</h1>

<p>Alright, we’re now ready to declare our fonts. In order to keep our code organized, we’ll add a new <code>fonts.css.scss</code> file to <code>app/assets/stylesheets</code>, and we’ll make our <code>@font-face</code> declarations right inside it. (Note that I use the SASS pre-compiler; hence, the additional <code>.scss</code> extension.)</p>

<p>Since I downloaded my fonts from Font Squirrel, I already have my font declarations pre-written for me. All I need to do is open the Webfont Kit I downloaded earlier, find the <code>stylesheet.css</code> file, and copy and paste the code into the <code>font.css.scss</code> file I just created above. If you didn’t use Font Squirrel, you’ll need to write the declarations yourself. You can follow the examples at <a href="https://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax">fontspring.com</a> for guidance.</p>

<p>I downloaded Font Squirrel’s &ldquo;Action Man&rdquo; font as an example, and below is the <code>@font-face</code> declaration that came with it.</p>

<p>``` scss fonts.css.scss
@font-face {</p>

<pre><code>font-family: 'action_manregular';
src: asset-url('Action_Man-webfont.eot');
src: asset-url('Action_Man-webfont.eot?#iefix') format('embedded-opentype'),
       asset-url('Action_Man-webfont.woff') format('woff'),
       asset-url('Action_Man-webfont.ttf') format('truetype'),
       asset-url('Action_Man-webfont.svg#action_manregular') format('svg');
font-weight: normal;
font-style: normal;
</code></pre>

<p>}
```</p>

<p>Note that the above declaration uses the <code>url()</code> methods by default to specify the font’s location. In order to make this work with the Rails Asset Pipeline, you’ll want to change those methods to <code>asset-url()</code>, or the fonts might not load.</p>

<p>Now all that&rsquo;s left is to declare our font inside whatever CSS file is appropriate using the <code>font-family</code> property, like so:</p>

<p><code>scss base.css.scss
p { font-family: 'action_manregular'; }
</code></p>

<p>Note that my font name matches the font name inside the <code>@font-face</code> <code>font-family</code> declaration. If names don’t match exactly, it might not work.</p>

<h1>And That&rsquo;s a Wrap</h1>

<p>Believe it or not, that’s all there is to it! If you refresh your browser, you should be able to see the new fonts in action. If that’s not the case, double check to make sure you’re using the <code>font-url()</code> methods if you’re running a pre-compiler like SASS or LESS, and make sure your <code>font-family</code> declarations match your <code>@font-face</code> declarations to the tee, including little things like capitalization, hyphens, underscores, etc. If that doesn&rsquo;t do it then Google might be your best bet.</p>
]]></content>
  </entry>
  
</feed>
